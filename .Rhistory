return(" ")
})
power <- tryCatch(
{
getPower(short_info)  # This will not throw an error
},
error = function() {
return(" ")
})
num_past_owners <- tryCatch(
{
getNumPastOwners(short_info)  # This will not throw an error
},
error = function() {
return(" ")
})
depreciation_val <- tryCatch(
{
getDepreciationVal(listing_page)  # This will not throw an error
},
error = function() {
return(" ")
})
reg_date <- tryCatch(
{
getRegDate(listing_page)  # This will not throw an error
},
error = function() {
return(" ")
})
coe_left <- tryCatch(
{
getCOELeft(listing_page)  # This will not throw an error
},
error = function() {
return(" ")
})
new_row <- data.frame(header = header, price = price, type = type, features = features, accessories = accessories,
description = description, categories = categories, mileage = mileage, road_tax = road_tax,
dereg_val = dereg_val, coe = coe, engine_cap = engine_cap, curb_weight = curb_weight,
manufactured_yr = manufactured_yr, transmission = transmission, omv = omv, arf = arf, power = power,
num_past_owners = num_past_owners, depreciation_val = depreciation_val, reg_date = reg_date, coe_left = coe_left)
return (new_row)
}
scrape_data <- function(page_number) {
retry <- TRUE
# Try to scrape data, with a maximum of 5 attempts
for (attempt in 1:5) {
if (!retry) break  # Exit the loop if retry is set to FALSE
tryCatch({
links_on_page <- store_links(page_number)
car_listings <- data.frame()
for (listing_link in links_on_page) {
car_listing <- get_info(listing_link)
car_listings <- rbind(car_listings, car_listing)
# Sleep for 2 seconds between requests
Sys.sleep(5)
}
# Set retry to FALSE when there is no error
retry <- FALSE
return(car_listings)
}, error = function(e) {
# Print error message
cat("Error on page", page_number, "attempt", attempt, ":", conditionMessage(e), "\n")
# Sleep for 10 minutes before retrying
cat("Sleeping for 10 minutes before retrying...\n")
Sys.sleep(600)
})
}
}
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
# Execute the scraping with foreach and doParallel
results_list <- foreach(i = 0:163, .packages = c("rvest", "httr")) %dopar% {
scrape_data(i)
}
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local", version = "3.4.0")
Sys.setenv(JAVA_HOME = "/Library/Java/JavaVirtualMachines/jdk-17.0.5.jdk/Contents/Home")
sc <- spark_connect(master = "local", version = "3.4.0")
# Define your schema
# schema <- list("INDEX" = "integer", "HEADER" = "character", "PRICE" = "double", "TYPE" = "character", "FEATURES" = "character", "ACCESSORIES" = "character", "DESCRIPTION" = "character", "CATEGORIES" = "character", "MILEAGE_KM" = "double", "ROAD_TAX" = "double", "DEREG_VAL" = "double", "COE_LISTED" = "double", "ENGINE_CAPACITY_CC" = "double", "CURB_WEIGHT_KG" = "double", "MANUFACTURED_YEAR" = "double", "TRANSMISSION" = "character", "OMV" = "double", "ARF" = "double", "POWER" = "double", "NUM_PAST_OWNERS" = "character", "DEPRE_VAL" = "double", "REG_DATE" = "character", "COE_TIME_LEFT" = "character")
# resale_cars_df <- spark_read_csv(sc, "sgCarMart_scraped.csv", escape = "\"", name = "resale_cars", columns = schema, na.strings = c("NA","N.A.","", " "))
# na_strings <- c("NA","N.A.","", " ")
scraped_df <- read.csv("sgCarMart_scraped.csv", na.strings = c("NA","N.A.","", " "), col.names = c("INDEX", "HEADER", "PRICE", "TYPE", "FEATURES", "ACCESSORIES", "DESCRIPTION", "CATEGORIES", "MILEAGE_KM", "ROAD_TAX", "DEREG_VAL", "COE_LISTED", "ENGINE_CAPACITY_CC", "CURB_WEIGHT_KG", "MANUFACTURED_YEAR", "TRANSMISSION", "OMV", "ARF", "POWER", "NUM_PAST_OWNERS", "DEPRE_VAL", "REG_DATE", "COE_TIME_LEFT"))
setwd("~/Documents/GitHub/bigdata-resalecaresinSG")
# Define your schema
# schema <- list("INDEX" = "integer", "HEADER" = "character", "PRICE" = "double", "TYPE" = "character", "FEATURES" = "character", "ACCESSORIES" = "character", "DESCRIPTION" = "character", "CATEGORIES" = "character", "MILEAGE_KM" = "double", "ROAD_TAX" = "double", "DEREG_VAL" = "double", "COE_LISTED" = "double", "ENGINE_CAPACITY_CC" = "double", "CURB_WEIGHT_KG" = "double", "MANUFACTURED_YEAR" = "double", "TRANSMISSION" = "character", "OMV" = "double", "ARF" = "double", "POWER" = "double", "NUM_PAST_OWNERS" = "character", "DEPRE_VAL" = "double", "REG_DATE" = "character", "COE_TIME_LEFT" = "character")
# resale_cars_df <- spark_read_csv(sc, "sgCarMart_scraped.csv", escape = "\"", name = "resale_cars", columns = schema, na.strings = c("NA","N.A.","", " "))
# na_strings <- c("NA","N.A.","", " ")
scraped_df <- read.csv("sgCarMart_scraped.csv", na.strings = c("NA","N.A.","", " "), col.names = c("INDEX", "HEADER", "PRICE", "TYPE", "FEATURES", "ACCESSORIES", "DESCRIPTION", "CATEGORIES", "MILEAGE_KM", "ROAD_TAX", "DEREG_VAL", "COE_LISTED", "ENGINE_CAPACITY_CC", "CURB_WEIGHT_KG", "MANUFACTURED_YEAR", "TRANSMISSION", "OMV", "ARF", "POWER", "NUM_PAST_OWNERS", "DEPRE_VAL", "REG_DATE", "COE_TIME_LEFT"))
resale_cars_df <- copy_to(sc, scraped_df, overwrite=TRUE)
resale_cars_df
# removing the first column as it is the index and categories column
resale_cars_df <- resale_cars_df |> select(-1,-CATEGORIES)
resale_cars_df <- resale_cars_df |>
mutate(
BRAND = substring_index(HEADER, ' ', 1),
MODEL_SUBMODEL = substring_index(HEADER, '(', 1),
MODEL_SUBMODEL = split(MODEL_SUBMODEL, " "),
MODEL_SUBMODEL = concat_ws(" ", slice(MODEL_SUBMODEL,2, size(MODEL_SUBMODEL)))) |>
select(BRAND, MODEL_SUBMODEL, everything(), -HEADER)
resale_cars_df
resale_cars_df|>
distinct(BRAND)
#Alfa -> Alfa Romeo, Mercedes -> Mercedes-Benz, Mini -> MINI, Land -> Land Rover, Aston -> Aston Martin, Rolls -> Rolls-Royce
resale_cars_df <- resale_cars_df |>
mutate(BRAND = ifelse(BRAND %in% "Mini", "MINI", BRAND)) |>
mutate(MODEL_SUBMODEL = ifelse(BRAND %in% c("Mercedes", "Alfa", "Land", "Rolls", "Aston"),
concat_ws(" ", slice(split(MODEL_SUBMODEL, " "), 2, size(split(MODEL_SUBMODEL, " ")))), MODEL_SUBMODEL)) |>
mutate(BRAND = ifelse(BRAND %in% "Mercedes", "Mercedes-Benz", BRAND),
BRAND = ifelse(BRAND %in% "Alfa", "Alfa Romeo", BRAND),
BRAND = ifelse(BRAND %in% "Land", "Land Rover", BRAND),
BRAND = ifelse(BRAND %in% "Aston", "Aston Martin", BRAND),
BRAND = ifelse(BRAND %in% "Rolls", "Rolls-Royce", BRAND)
)
#checking if the brand names have been changed
resale_cars_df |>
filter(BRAND %in% c("Mercedes", "Alfa", "Land", "Rolls", "Aston", "Mini"))
resale_cars_df |>
filter(BRAND %in% c("Mercedes-Benz", "Alfa Romeo", "Land Rover", "Rolls-Royce", "Aston Martin", "MINI"))
### GROUPING THE BRANDS INTO BROADER CATEGORIES: EXOTIC, ULTA LUX, LUX, MID-LEVEL, ECONOMY, BUDGET, OTHERS
## OBTAINING A LIST OF RARE BRANDS -- 'OTHERS'
# Calculate the count for each brand
brand_counts <- resale_cars_df |>
group_by(BRAND) |>
summarise(count = n())
# Identify brands with count <= 5 and saving it as a vector
rare_brands <- brand_counts |>
filter(count <= 5) |>
select(BRAND) |>
collect() |>
pull(BRAND)
## new column called new_brand to identify the groupings. so far only others.
resale_cars_df <- resale_cars_df |>
mutate(BRAND_CATEGORY = ifelse(BRAND %in% rare_brands, 'Others', BRAND))
# Define vectors for each category
exotic_brands <- c("Aston Martin", "Ferrari", "Lamborghini", "McLaren")
ultra_luxury_brands <- c("Bentley", "Land Rover", "Maserati", "Porsche", "Rolls-Royce", "Alpine")
luxury_brands <- c("Audi", "BMW", "Jaguar", "Jeep", "Lexus", "Lotus", "Mercedes-Benz", "Volvo", "Polestar", "Tesla")
mid_level_brands <- c("Alfa Romeo", "Infiniti", "MINI", "Opel", "Volkswagen", "BYD", "MG")
economy_brands <- c("Chevrolet", "Citroen", "Fiat", "Ford", "Honda", "Hyundai", "Kia", "Mazda", "Mitsubishi", "Nissan", "Peugeot", "Renault", "Skoda", "Ssangyong", "Subaru", "Suzuki", "Toyota", "Smart", "SEAT", "Austin", "Morris")
budget_brands <- c("Proton", "Perodua")
# Create a new column "new_brand" based on the category
resale_cars_df <- resale_cars_df |>
mutate(BRAND_CATEGORY = case_when(
BRAND_CATEGORY != 'Others' ~
case_when(
BRAND %in% exotic_brands ~ 'Exotic',
BRAND %in% ultra_luxury_brands ~ 'Ultra Luxury',
BRAND %in% luxury_brands ~ 'Luxury',
BRAND %in% mid_level_brands ~ 'Mid Level',
BRAND %in% economy_brands ~ 'Economy',
BRAND %in% budget_brands ~ 'Budget',
TRUE ~ 'Others'
),
TRUE ~ 'Others'
))
resale_cars_df <- resale_cars_df |>
mutate(
DAYS = as.integer(regexp_extract(COE_TIME_LEFT, "(\\\\d+)day", 1)),
MONTHS = as.integer(regexp_extract(COE_TIME_LEFT, "(\\\\d+)mth", 1)),
YEARS = as.integer(regexp_extract(COE_TIME_LEFT, "(\\\\d+)yr", 1)),
DAYS_OF_COE_LEFT = ifelse(isnull(DAYS),0,DAYS) + ifelse(isnull(MONTHS),0,MONTHS)*30 + ifelse(isnull(YEARS),0,YEARS)*365) |>
select(-DAYS,-MONTHS,-YEARS,-COE_TIME_LEFT)
resale_cars_df <- resale_cars_df |>
mutate(
MANUFACTURED_YEAR = ifelse(MANUFACTURED_YEAR >= 2023, NA, MANUFACTURED_YEAR),
CURB_WEIGHT_KG = ifelse(CURB_WEIGHT_KG < 100, NA, CURB_WEIGHT_KG),
DAYS_OF_COE_LEFT = ifelse(DAYS_OF_COE_LEFT > 3650 | DAYS_OF_COE_LEFT <= 0, NA, DAYS_OF_COE_LEFT))
##imputing CURB_WEIGHT, ENGINE CAPACITY and POWER according to model_submodel + manufactured year
# Step 1: Group by MODEL_SUBMODEL and MANUFACTURED_YEAR and calculate the average values
average_values <- resale_cars_df |>
group_by(MODEL_SUBMODEL, MANUFACTURED_YEAR) |>
summarize(
AVG_CURB_WEIGHT_KG = mean(CURB_WEIGHT_KG, na.rm = TRUE),
AVG_ENGINE_CAPACITY_CC = mean(ENGINE_CAPACITY_CC, na.rm = TRUE),
AVG_POWER = mean(POWER, na.rm = TRUE),
MOST_COMMON_TYPE = mode(TYPE)
)
# Step 2: Join the original DataFrame with the average values based on MODEL_SUBMODEL and MANUFACTURED_YEAR
resale_cars_df <- resale_cars_df |>
left_join(average_values, by = c("MODEL_SUBMODEL", "MANUFACTURED_YEAR"))
# Step 3: Replace missing values with the corresponding average values
resale_cars_df <- resale_cars_df |>
mutate(
CURB_WEIGHT_KG = ifelse(is.na(CURB_WEIGHT_KG), AVG_CURB_WEIGHT_KG, CURB_WEIGHT_KG),
ENGINE_CAPACITY_CC = ifelse(is.na(ENGINE_CAPACITY_CC), AVG_ENGINE_CAPACITY_CC, ENGINE_CAPACITY_CC),
POWER = ifelse(is.na(POWER), AVG_POWER, POWER),
TYPE = ifelse(is.na(TYPE), MOST_COMMON_TYPE, TYPE)
) |>
select(-AVG_CURB_WEIGHT_KG, -AVG_ENGINE_CAPACITY_CC, -AVG_POWER, -MOST_COMMON_TYPE)  # Remove the temporary columns
#minus 1 because all of them end with "see specifications" as the last substring, which is not a feature.
resale_cars_df <- resale_cars_df |> mutate(FEATURE_COUNT = size(split(FEATURES, '[.,]'))-1,
ACCESSORIES_COUNT = size(split(ACCESSORIES, '[.,]'))
) |>
select(-FEATURES,-ACCESSORIES)
resale_cars_df <- resale_cars_df |> mutate(AGE_SINCE_MANUFACTURED = as.numeric(2023 - MANUFACTURED_YEAR),
DAYS_SINCE_REGISTERED = as.numeric(datediff(to_date("14-Nov-2023", "dd-MMM-yyyy"), to_date(REG_DATE, "dd-MMM-yyyy")))) |>
select(-MANUFACTURED_YEAR, -REG_DATE)
#change the character string "More than 6" in num_past_owners to 7 and make the column into integer type
resale_cars_df <- resale_cars_df |> mutate(NUM_PAST_OWNERS = ifelse(NUM_PAST_OWNERS == "More than 6", 7, NUM_PAST_OWNERS)) |>
mutate(NUM_PAST_OWNERS = as.integer(NUM_PAST_OWNERS))
## next we remove all rows with any missing values
resale_cars_df <- na.omit(resale_cars_df)
## save the final dataset as a csv
write.csv(resale_cars_df, "cleaned_resalecars.csv", row.names=FALSE)
## we also save a parquet file
spark_write_parquet(resale_cars_df, 'cleaned_resalecars.parquet')
setwd("/Users/kowsalya/Documents/GitHub/bigdata-resalecaresinSG")
## save the final dataset as a csv
write.csv(resale_cars_df, "cleaned_resalecars.csv", row.names=FALSE)
## we also save a parquet file
spark_write_parquet(resale_cars_df, 'cleaned_resalecars.parquet')
library(ggplot2)
library(dplyr)
library(dbplot)
library(corrplot)
library(corrr)
df <- spark_read_parquet(sc, "cleaned_resalecars.parquet")
df <- spark_read_parquet(sc, "cleaned_resalecars.parquet")
# Correlation matrix w/ all numerical vars:
df_numeric <- df |>
select(PRICE, MILEAGE_KM, DEPRE_VAL, DEREG_VAL, ROAD_TAX, COE_LISTED, ENGINE_CAPACITY_CC, CURB_WEIGHT_KG, OMV, ARF, POWER, NUM_PAST_OWNERS, DAYS_OF_COE_LEFT, FEATURE_COUNT, ACCESSORIES_COUNT, AGE_SINCE_MANUFACTURED, DAYS_SINCE_REGISTERED) |>
collect()
glimpse(df_numeric)
corrplot(cor(df_numeric), method = "color", type = "lower", tl.cex = 0.5, tl.col = "black", tl.srt = 45, addCoef.col = "black", number.cex = 0.4)
# Correlation matrix w/ the final numerical variables + check for more correlation (< 0.9) between variables (none)
df_second <- df_numeric |>
select(PRICE, MILEAGE_KM, CURB_WEIGHT_KG, OMV, NUM_PAST_OWNERS, COE_LISTED, ENGINE_CAPACITY_CC, DAYS_OF_COE_LEFT, FEATURE_COUNT, ACCESSORIES_COUNT, AGE_SINCE_MANUFACTURED, DAYS_SINCE_REGISTERED)
glimpse(df_second)
corrplot(cor(df_second), method = "color", type = "lower", tl.cex = 0.5, tl.col = "black", tl.srt = 45, addCoef.col = "black", number.cex = 0.6)
df_chr <- df |>
select(PRICE, TYPE, TRANSMISSION, BRAND_CATEGORY) |>
collect()
glimpse(df_chr)
# BAR PLOTS
# PRICE against BRAND CATEGORY
ggplot(df_chr, aes(x = BRAND_CATEGORY, y = PRICE)) +
geom_bar(fill = "orange", stat = "summary", fun = "mean") +
labs(x = "Category of Car Brands", y = "Average Price in Thousands") +
scale_y_continuous(breaks = seq(0, 2000000, by = 100000),
labels = scales::comma_format(scale = 1e-3))
# PRICE against TYPE
ggplot(df_chr, aes(x = TYPE, y = PRICE)) +
geom_bar(fill = "orange", stat = "summary", fun = "mean") +
labs(x = "Car Types", y = "Average Price in Thousands") +
scale_y_continuous(breaks = seq(0, 2000000, by = 100000),
labels = scales::comma_format(scale = 1e-3))
# PRICE against TRANSMISSION
ggplot(df_chr, aes(x = TRANSMISSION, y = PRICE)) +
geom_bar(fill = "orange", stat = "summary", fun = "mean") +
labs(x = "Car Transmission", y = "Average Price in Thousands")
# RASTER PLOTS
# PRICE against MILEAGE_KM
df_second |>
dbplot_raster(x = MILEAGE_KM, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("Mileage in Thousands Kilometre",
breaks = seq(0, 2000000, by = 100000),
labels = scales::comma_format(scale = 1e-3)) +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# PRICE against NUM_PAST_OWNERS
df_second |>
dbplot_raster(x = NUM_PAST_OWNERS, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("Number of Owners") +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# PRICE against OMV
df_second |>
dbplot_raster(x = OMV, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("Open Market Value in Thousands",
breaks = seq(0, 2000000, by = 100000),
labels = scales::comma_format(scale = 1e-3)) +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# PRICE against COE_LISTED
df_second |>
dbplot_raster(x = COE_LISTED, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("COE Listed") +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# PRICE against ENGINE_CAPACITY_CC
df_second |>
dbplot_raster(x = ENGINE_CAPACITY_CC, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("Engine Capacity") +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# PRICE against CURB_WEIGHT_KG
df_second |>
dbplot_raster(x = CURB_WEIGHT_KG, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("Curb Weight in Kilograms") +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# PRICE against DAYS_OF_COE_LEFT
df_second |>
dbplot_raster(x = DAYS_OF_COE_LEFT, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("Days of COE Left") +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# PRICE against FEATURE_COUNT
df_second |>
dbplot_raster(x = FEATURE_COUNT, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("Number of Features") +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# PRICE against ACCESSORIES_COUNT
df_second |>
dbplot_raster(x = ACCESSORIES_COUNT, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("Number of Accessories") +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# PRICE against AGE_SINCE_MANUFACTURED
df_second |>
dbplot_raster(x = AGE_SINCE_MANUFACTURED, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("Car's age since manufactured") +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# PRICE against DAYS_SINCE_REGISTERED
df_second |>
dbplot_raster(x = DAYS_SINCE_REGISTERED, y = PRICE, fill = n(), resolution = 10) +
scale_x_continuous("Number of days since car is registered") +
scale_y_continuous("Price in Millions",
breaks = seq(0, 3000000, by = 500000),
labels = scales::comma_format(scale = 1e-6))
# Final dataset with confirmed predictors: 11 numerical + 3 categorical
df_final <- df |>
select(PRICE, TYPE, TRANSMISSION, COE_LISTED, NUM_PAST_OWNERS, BRAND_CATEGORY, MILEAGE_KM, CURB_WEIGHT_KG, OMV, ENGINE_CAPACITY_CC, DAYS_OF_COE_LEFT, FEATURE_COUNT, ACCESSORIES_COUNT, AGE_SINCE_MANUFACTURED, DAYS_SINCE_REGISTERED)
#read parquet file as df
df_parquet <- spark_read_parquet(sc, "cleaned_resalecars.parquet")
pipeline <- ml_pipeline(sc) |>
ft_vector_assembler(
input_cols = c("MILEAGE_KM", "CURB_WEIGHT_KG", "OMV", "COE_LISTED", "ENGINE_CAPACITY_CC", "AGE_SINCE_MANUFACTURED", "DAYS_SINCE_REGISTERED", "NUM_PAST_OWNERS", "DAYS_OF_COE_LEFT", "FEATURE_COUNT", "ACCESSORIES_COUNT"),
output_col = "features"
) |>
ft_standard_scaler(
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
) |>
ft_string_indexer(
input_col = "TYPE",
output_col = "TYPE_index"
) |>
ft_string_indexer(
input_col = "TRANSMISSION",
output_col = "TRANSMISSION_index"
) |>
ft_string_indexer(
input_col = "BRAND_CATEGORY",
output_col = "BRAND_CATEGORY_index"
) |>
ft_vector_assembler(
input_cols = c("features_scaled", "TYPE_index", "TRANSMISSION_index", "BRAND_CATEGORY_index"),
output_col = "final_features"
) |>
ml_linear_regression(
features_col = "final_features",
label_col = "PRICE"
)
pipeline
#Cross validation: Split data into train and test sets
df_split <- df_parquet |>
sdf_random_split(training = 0.8, testing = 0.2, seed = 1234)
df_train <- df_split$training
df_test <- df_split$testing
#Fitting pipeline to model
pipeline_model <- ml_fit(pipeline, df_train)
pipeline_model
# Get the linear regression model from the fitted pipeline
lr_model <- pipeline_model$stages[[length(pipeline_model$stages)]]
# Access the coefficients of the linear regression model
coefficients <- coefficients(lr_model)
coefficients
#Cross validate pipeline_model
cv <- ml_cross_validator(
sc,
estimator = pipeline,
estimator_param_maps = list(
linear_regression = list(
elastic_net_param = c(0,1),
reg_param = c(0.1, 0.005, 0.001)
)
),
evaluator = ml_regression_evaluator(
sc,
label_col = "PRICE"
),
num_folds = 10,
parallelism = 1,
seed = 123
)
cv
#Validating best model.
cv_model <- ml_fit(cv, df_train)
#Validating best model.
cv_model <- ml_fit(cv, df_train)
library(dplyr)
library(readr)
df <- read_csv("cleaned_resalecars.csv")
library(sparklyr)
sc <- spark_connect(master = "local", version = "3.4.0")
#read parquet file as df
df_parquet <- spark_read_parquet(sc, "cleaned_resalecars.parquet")
#pipeline 2 (with text-based features)
pipeline <- ml_pipeline(sc) |>
ft_vector_assembler(
input_cols = c("MILEAGE_KM", "CURB_WEIGHT_KG", "OMV", "COE_LISTED", "ENGINE_CAPACITY_CC", "AGE_SINCE_MANUFACTURED", "DAYS_SINCE_REGISTERED", "NUM_PAST_OWNERS", "DAYS_OF_COE_LEFT", "FEATURE_COUNT", "ACCESSORIES_COUNT"),
output_col = "features"
) |>
ft_standard_scaler(
input_col = "features",
output_col = "features_scaled",
with_mean = TRUE
) |>
ft_string_indexer(
input_col = "TYPE",
output_col = "TYPE_index"
) |>
ft_string_indexer(
input_col = "TRANSMISSION",
output_col = "TRANSMISSION_index"
) |>
ft_string_indexer(
input_col = "BRAND_CATEGORY",
output_col = "BRAND_CATEGORY_index"
) |>
ft_vector_assembler(
input_cols = c("features_scaled", "TYPE_index", "TRANSMISSION_index", "BRAND_CATEGORY_index"),
output_col = "final_features"
) |>
ml_linear_regression(
features_col = "final_features",
label_col = "PRICE"
)
pipeline
#Cross validation: Split data into train and test sets
df_split <- df_parquet |>
sdf_random_split(training = 0.8, testing = 0.2, seed = 1234)
df_train <- df_split$training
df_test <- df_split$testing
#Fitting pipeline to model
pipeline_model <- ml_fit(pipeline, df_train)
pipeline_model
# Get the linear regression model from the fitted pipeline
lr_model <- pipeline_model$stages[[length(pipeline_model$stages)]]
# Access the coefficients of the linear regression model
coefficients <- coefficients(lr_model)
coefficients
#Cross validate pipeline_model
cv <- ml_cross_validator(
sc,
estimator = pipeline,
estimator_param_maps = list(
linear_regression = list(
elastic_net_param = c(0,1),
reg_param = c(0.1, 0.005, 0.001)
)
),
evaluator = ml_regression_evaluator(
sc,
label_col = "PRICE"
),
num_folds = 10,
parallelism = 1,
seed = 123
)
cv
#Validating best model.
cv_model <- ml_fit(cv, df_train)
#Cross validate pipeline_model
cv <- ml_cross_validator(
sc,
estimator = pipeline,
estimator_param_maps = list(
linear_regression = list(
elastic_net_param = c(0,1),
reg_param = c(0.1, 0.005, 0.001)
)
),
evaluator = ml_regression_evaluator(
sc,
label_col = "PRICE"
),
num_folds = 10,
parallelism = 6,
seed = 123
)
cv
#Validating best model.
cv_model <- ml_fit(cv, df_train)
# Get the linear regression model from the fitted pipeline
lr_model <- pipeline_model$stages[[length(pipeline_model$stages)]]
# Access the coefficients of the linear regression model
coefficients <- coefficients(lr_model)
coefficients
# Get the linear regression model from the fitted pipeline
lr_model <- pipeline_model$stages[[length(pipeline_model$stages)]]
# Access the coefficients of the linear regression model
coefficients <- coefficients(lr_model)
coefficients
